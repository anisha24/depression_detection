{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFKxDUOmBlDR"
      },
      "source": [
        "**Imports**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gAhtrXkd6Y1I",
        "outputId": "3a12fdbc-37d6-4595-bdd2-4b896f578150"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBb6KEjI6fmD",
        "outputId": "d3db49dd-04a9-455e-8019-971a7c2dc0da"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "In_lN5Ch6pKD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gc\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from smart_open import open\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.metrics import classification_report\n",
        "from gensim.models.keyedvectors import KeyedVectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuVdNjscBprS"
      },
      "source": [
        "**Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LAR4Bf3E6tCg"
      },
      "outputs": [],
      "source": [
        "# reading the train, text and dev datasets\n",
        "dataset_1 = np.array(pd.read_csv('/content/drive/My Drive/ICAIML_Project/Datasets/diacwoz/dev_split_Depression_AVEC2017.csv',delimiter=',',encoding='utf-8'))[:, 0:2]\n",
        "dataset_2 = np.array(pd.read_csv('/content/drive/My Drive/ICAIML_Project/Datasets/diacwoz/full_test_split.csv',delimiter=',',encoding='utf-8'))[:, 0:2]\n",
        "dataset_3 = np.array(pd.read_csv('/content/drive/My Drive/ICAIML_Project/Datasets/diacwoz/train_split_Depression_AVEC2017.csv',delimiter=',',encoding='utf-8'))[:, 0:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DdrOxkzM6zan"
      },
      "outputs": [],
      "source": [
        "# concatenating the full dataset\n",
        "full_dataset = np.concatenate((dataset_1, np.concatenate((dataset_2, dataset_3))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "CikCKEUZ7d87"
      },
      "outputs": [],
      "source": [
        "def checkBoolVal(dataset, index):\n",
        "    for i in range(len(dataset)):\n",
        "        if(dataset[i][0] == index):\n",
        "            return dataset[i][1]\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "k8QUrWLJ8BJe"
      },
      "outputs": [],
      "source": [
        "# initializing all variables\n",
        "Data = []\n",
        "Y = []\n",
        "Data_test = []\n",
        "Y_test = []\n",
        "index = -1\n",
        "\n",
        "# getting transcript files for each participant\n",
        "for i in range(len(dataset_3)):\n",
        "    val = checkBoolVal(full_dataset, dataset_3[i][0])\n",
        "    Y.append(val)\n",
        "    try:\n",
        "        train_data_fileName = \"/content/drive/My Drive/ICAIML_Project/Datasets/diacwoz/train_data/\" + str(int(dataset_3[i][0])) + \"_TRANSCRIPT.csv\"\n",
        "        Data.append(np.array(pd.read_csv(train_data_fileName, delimiter='\\t', encoding='utf-8'))[:, 2:4])\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "for i in range(len(dataset_1)):\n",
        "    val = checkBoolVal(full_dataset, dataset_1[i][0])\n",
        "    Y.append(val)\n",
        "    try:\n",
        "        dev_data_fileName = \"/content/drive/My Drive/ICAIML_Project/Datasets/diacwoz/dev_data/\" + str(int(dataset_1[i][0])) + \"_TRANSCRIPT.csv\"\n",
        "        Data.append(np.array(pd.read_csv(dev_data_fileName, delimiter='\\t', encoding='utf-8'))[:, 2:4])\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "for i in range(0, len(dataset_2)):\n",
        "    Y_test.append(checkBoolVal(full_dataset, dataset_2[i][0]))\n",
        "    try:\n",
        "        test_data_fileName = \"/content/drive/My Drive/ICAIML_Project/Datasets/diacwoz/test_data/\" + str(int(dataset_2[i][0])) + \"_TRANSCRIPT.csv\"\n",
        "        Data_test.append(np.array(pd.read_csv(test_data_fileName, delimiter='\\t', encoding='utf-8'))[:, 2:4])\n",
        "    except Exception as e:\n",
        "        print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "010ktnsW8cwO"
      },
      "outputs": [],
      "source": [
        "# appending only participant value to Data2\n",
        "Y = np.array(Y)\n",
        "Data2 = []\n",
        "\n",
        "Data2_test = []\n",
        "Y_test = np.array(Y_test)\n",
        "\n",
        "# for training data\n",
        "for i in range(len(Data)):\n",
        "    script = []\n",
        "    for k in range(1, len(Data[i])):\n",
        "        if(Data[i][k][0] == \"Participant\"):\n",
        "            script.append(Data[i][k][1])\n",
        "    Data2.append(script)\n",
        "    \n",
        "# for test data\n",
        "for i in range(len(Data_test)):\n",
        "    script = []\n",
        "    for k in range(1, len(Data_test[i])):\n",
        "        if(Data_test[i][k][0] == \"Participant\"):\n",
        "            script.append(Data_test[i][k][1])\n",
        "    Data2_test.append(script)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "IikpxtZg8jFt"
      },
      "outputs": [],
      "source": [
        "Data = []\n",
        "Data_test = []\n",
        "\n",
        "# running garbage collection to free up memory\n",
        "gc.collect()   \n",
        "\n",
        "Data2 = np.array(Data2, dtype=object)\n",
        "Data2_test = np.array(Data2_test, dtype=object)\n",
        "\n",
        "# loading GoogleNews keyed vectors bin file\n",
        "model = KeyedVectors.load_word2vec_format('/content/drive/MyDrive/ICAIML_Project/Datasets/GoogleNews-vectors-negative300.bin', binary=True)\n",
        "\n",
        "# getting the stop words\n",
        "stop_words = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "-7D84k5D8-LL"
      },
      "outputs": [],
      "source": [
        "def threshold(Y_pred, t):\n",
        "    Y_pred2 = []\n",
        "    for i in range(len(Y_pred)):\n",
        "        if(Y_pred[i] < t):\n",
        "            Y_pred2.append(0)\n",
        "        else:\n",
        "            Y_pred2.append(1)\n",
        "    return np.array(Y_pred2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "BmR5eZwY9Avw"
      },
      "outputs": [],
      "source": [
        "def rem_stopwords(sent):\n",
        "    filt_sent = [] \n",
        "    for word in sent: \n",
        "        if word not in stop_words: \n",
        "            filt_sent.append(word)\n",
        "    return filt_sent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ltZo_Ta79Kks"
      },
      "outputs": [],
      "source": [
        "# upsampling training data\n",
        "def upsample(X_train,Y_train):\n",
        "    X_train_0 = X_train[Y_train==0]\n",
        "    X_train_1 = X_train[Y_train==1]\n",
        "\n",
        "    # print(X_train_0.shape)\n",
        "    Y_train_1 = Y_train[Y_train==1]\n",
        "    size = X_train_0.shape[0] - X_train_1.shape[0]\n",
        "    X = []\n",
        "    Y = []\n",
        "    X_train = list(X_train)\n",
        "    Y_train = list(Y_train)\n",
        "\n",
        "    while(size>0):\n",
        "        size -= 1\n",
        "        index = np.random.randint(0,X_train_1.shape[0]-1)\n",
        "        leave_index = np.random.randint(0,len(X_train)-1)\n",
        "        X_add = X_train_1[index]\n",
        "        X_leave = X_train[leave_index]\n",
        "\n",
        "        Y_add = Y_train_1[index]\n",
        "        Y_leave = Y_train[leave_index]\n",
        "\n",
        "        X_train[leave_index] = X_add\n",
        "        X_train.append(X_leave)\n",
        "\n",
        "        Y_train[leave_index] = Y_add\n",
        "        Y_train.append(Y_leave)\n",
        "\n",
        "\n",
        "    X_train = np.array(X_train)\n",
        "    Y_train = np.array(Y_train)\n",
        "    return X_train,Y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "jnsPmlI77AB_"
      },
      "outputs": [],
      "source": [
        "max_ws = 20\n",
        "max_sent = 250\n",
        "\n",
        "# preprocessing the train data\n",
        "matrix = np.zeros((Data2.shape[0], max_sent, max_ws, 300))\n",
        "max_sent_len = 0\n",
        "sent = \"\"\n",
        "for k in range(Data2.shape[0]):\n",
        "    if(max_sent_len < len(Data2[k])):\n",
        "        max_sent_len = len(Data2[k])\n",
        "        sent = Data2[k]\n",
        "    for i in range(min(max_sent, len(Data2[k]))):\n",
        "        try:\n",
        "            sent = Data2[k][i].split(\" \")\n",
        "        except:\n",
        "            continue\n",
        "        sent = rem_stopwords(sent)\n",
        "        for j in range(min(max_ws, len(sent))):\n",
        "            try:\n",
        "                # removing the < and > from the words\n",
        "                w = sent[j]\n",
        "                if(w[0] == '<'):\n",
        "                    if(w.find('>')!=-1):\n",
        "                        w = w[1:-1]\n",
        "                    else:\n",
        "                        w = w[1:]\n",
        "                else:\n",
        "                    if(w.find('>')!=-1):\n",
        "                        w = w[0:-1]\n",
        "                matrix[k][i][j] = np.array(model[w])\n",
        "            except Exception as e:\n",
        "                continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "c1rdhnHC9Pdv"
      },
      "outputs": [],
      "source": [
        "# preprocessing the test data\n",
        "# separated due to memory constraints\n",
        "max_len_sent = 0\n",
        "matrix_test = np.zeros((Data2_test.shape[0], max_sent, max_ws, 300))\n",
        "for k in range(Data2_test.shape[0]):\n",
        "    if(max_len_sent < len(Data2_test[k])):\n",
        "      max_len_sent = len(Data2_test[k])\n",
        "      sent = Data2_test[k]\n",
        "    for i in range(min(max_sent, len(Data2_test[k]))):\n",
        "        try:\n",
        "            sent = Data2_test[k][i].split(\" \")\n",
        "        except:\n",
        "            continue\n",
        "        sent = rem_stopwords(sent)\n",
        "        for j in range(min(max_ws, len(sent))):\n",
        "            try:\n",
        "                # removing the < and > from the words\n",
        "                w = sent[j]\n",
        "                if(w[0] == '<'):\n",
        "                    if(w.find('>')!=-1):\n",
        "                        w = w[1:-1]\n",
        "                    else:\n",
        "                        w = w[1:]\n",
        "                else:\n",
        "                    if(w.find('>')!=-1):\n",
        "                        w = w[0:-1]\n",
        "                matrix_test[k][i][j] = np.array(model[w])\n",
        "            except Exception as e:\n",
        "                continue\n",
        "\n",
        "Data2 = []\n",
        "Data2_test = []\n",
        "model = []\n",
        "stop_words = []\n",
        "\n",
        "# running garbage collection to free up memory\n",
        "gc.collect()\n",
        "\n",
        "# upsampling training and test data\n",
        "matrix, Y = upsample(matrix,Y)\n",
        "matrix_test, Y_test = upsample(matrix_test,Y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJLR3KLGBucE"
      },
      "source": [
        "**CNN Class**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iob9dcjB0gj",
        "outputId": "21ce3c6b-943b-408c-9d7c-db8a09b52a6f"
      },
      "outputs": [],
      "source": [
        "class CNN:\n",
        "  def __init__(self):\n",
        "    classifier = Sequential()\n",
        "    classifier.add(Conv2D(150, (1, 5), input_shape = (matrix.shape[1], matrix.shape[2], matrix.shape[3]), activation = 'relu', data_format=\"channels_last\"))\n",
        "    classifier.add(MaxPooling2D(pool_size = (1, 3)))\n",
        "    classifier.add(Conv2D(75, (1, 3), activation = 'relu', data_format=\"channels_last\"))\n",
        "    classifier.add(MaxPooling2D(pool_size = (1, 2)))\n",
        "    classifier.add(Flatten())\n",
        "    classifier.add(Dense(units = 128, activation = 'relu'))\n",
        "    classifier.add(Dense(units = 1, activation = 'sigmoid'))\n",
        "    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "    self.classifier = classifier\n",
        "\n",
        "  def fitModel(self, X, Y, epoch):\n",
        "    class_weight = {0: 0.4, 1: 0.7}\n",
        "    self.classifier.fit(X, Y, epochs = epoch, class_weight=class_weight)\n",
        "\n",
        "  def predictModel(self, X):\n",
        "    return threshold(self.classifier.predict(X), 0.45)\n",
        "  \n",
        "model = CNN()\n",
        "model.fitModel(matrix, Y, 5)\n",
        "Y_Pred = model.predictModel(matrix_test)\n",
        "print(classification_report(Y_test, Y_Pred))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
